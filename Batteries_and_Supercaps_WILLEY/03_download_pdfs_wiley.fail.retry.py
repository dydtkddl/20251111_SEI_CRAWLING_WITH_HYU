# -*- coding: utf-8 -*-
"""
03_download_pdfs_wiley_retry_from_txt.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… failed_dois.txt ê¸°ë°˜ ì¬ì‹œë„ ë‹¤ìš´ë¡œë“œ
âœ… PDF magic number + HTML fallback ê°ì§€
âœ… cookies.txt ê¸°ë°˜ (Netscape format)
âœ… curl ì‚¬ìš© (TLS/ë¸Œë¼ìš°ì € í˜¸í™˜)
âœ… ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ + tqdm
"""

import os
import re
import time
import argparse
import logging
import subprocess
from multiprocessing import Pool
from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging setup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler("wiley_retry.log"),
        logging.StreamHandler(),
    ],
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE = "https://chemistry-europe.onlinelibrary.wiley.com/doi/pdfdirect/"
UA = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
REFERER = "https://chemistry-europe.onlinelibrary.wiley.com/"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sanitize(text: str) -> str:
    """íŒŒì¼ ì´ë¦„ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°"""
    return re.sub(r'[\\/*?:"<>|]', "_", text)

def check_pdf_valid(filepath):
    """PDF magic number ë° HTML fallback ê°ì§€"""
    try:
        with open(filepath, "rb") as f:
            head = f.read(200)
        if b"%PDF" in head and b"<html" not in head.lower():
            return True
        os.remove(filepath)
        return False
    except Exception:
        return False

def run_curl(cmd):
    return subprocess.run(cmd, shell=True, stderr=subprocess.DEVNULL).returncode == 0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Worker
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_one(row):
    idx, doi, cookies, outdir = row
    pdf_url = f"{BASE}{doi}?download=true"
    outfile = os.path.join(outdir, f"{sanitize(doi)}.pdf")

    # ì´ë¯¸ ì„±ê³µí•œ íŒŒì¼ ì¡´ì¬ ì‹œ skip
    if os.path.exists(outfile) and os.path.getsize(outfile) > 10_000 and check_pdf_valid(outfile):
        return {"idx": idx, "doi": doi, "status": "exists"}

    cmd = (
        f'curl -s -L -b "{cookies}" '
        f'-A "{UA}" '
        f'-e "{REFERER}" '
        f'-H "Accept: application/pdf,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" '
        f'-o "{outfile}" "{pdf_url}"'
    )

    for attempt in range(3):
        run_curl(cmd)
        if check_pdf_valid(outfile):
            logging.info(f"âœ… [{idx}] OK â†’ {outfile}")
            return {"idx": idx, "doi": doi, "status": "success"}
        else:
            logging.warning(f"âš ï¸ [{idx}] {doi} invalid/403 (attempt {attempt+1}/3)")
            time.sleep(2)

    return {"idx": idx, "doi": doi, "status": "failed"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--txt", default="failed_dois.txt", help="TXT file containing DOIs (one per line)")
    ap.add_argument("--cookies", default="cookies.txt", help="cookies.txt (Netscape format)")
    ap.add_argument("--out", default="wiley_pdfs_retry", help="Output directory for retried PDFs")
    ap.add_argument("--n_cpus", type=int, default=6, help="Number of parallel workers")
    args = ap.parse_args()

    if not os.path.exists(args.txt):
        raise FileNotFoundError(f"DOI list file not found: {args.txt}")
    if not os.path.exists(args.cookies):
        raise FileNotFoundError(f"Cookies file not found: {args.cookies}")

    os.makedirs(args.out, exist_ok=True)

    # TXT ì½ê¸°
    with open(args.txt, "r", encoding="utf-8") as f:
        dois = [line.strip() for line in f if line.strip().startswith("10.")]

    data = [(i + 1, doi, args.cookies, args.out) for i, doi in enumerate(dois)]

    logging.info(f"ğŸš€ Starting {len(data)} verified re-downloads with {args.n_cpus} workers")
    results = []
    with Pool(args.n_cpus) as pool:
        for res in tqdm(pool.imap_unordered(download_one, data), total=len(data), desc="ğŸ“¥ Wiley Retry PDFs"):
            results.append(res)

    # ê²°ê³¼ ì €ì¥
    import pandas as pd
    pd.DataFrame(results).to_csv("wiley_retry_results.csv", index=False)
    logging.info("ğŸ§¾ Saved â†’ wiley_retry_results.csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    main()

